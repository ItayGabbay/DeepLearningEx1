import numpy as np
from feedforward import ActivationFunctionNotFound


def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer

    :param dZ: The gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: Tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    dA_prev = np.dot(W.T, dZ)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function
    first computes dZ and then applies the linear_backward function.

    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: which activation function was used in feedforward
    :return:
        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
    """

    linear_cache, activation_cache = cache

    if activation == 'relu':
        dZ = relu_backward(dA, activation_cache)
    elif activation == 'sigmoid':
        dZ = sigmoid_backward(dA, activation_cache)
    else:
        raise ActivationFunctionNotFound

    dA_prev, dW, db = Linear_backward(dZ, linear_cache)
    return dA_prev, dW, db


def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation process for the entire network.

    :param AL: The probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: The true labels vector (the "ground truth" - true classifications)
    :param caches: List of caches containing for each layer: a) the linear cache; b) the activation cache
    :return:
        Grads - a dictionary with the gradients

    """

    Grads = {}
    num_layers = len(caches)

    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    dAOutput, dWOutput, dbOutput = linear_activation_backward(dAL, caches[num_layers - 1], activation="sigmoid")
    Grads["dA" + str(num_layers)] = dAOutput
    Grads["dW" + str(num_layers)] = dWOutput
    Grads["db" + str(num_layers)] = dbOutput

    for layer in reversed(range(num_layers - 1)):
        cache = caches[layer]
        dA_prev, dW, db = linear_activation_backward(Grads["dA" + str(layer + 2)], cache, activation="relu")
        Grads["dA" + str(layer + 1)] = dA_prev
        Grads["dW" + str(layer + 1)] = dW
        Grads["db" + str(layer + 1)] = db

    return Grads


def Update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent

    :param parameters: A python dictionary containing the DNN architecture’s parameters
    :param grads: A python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: The learning rate used to update the parameters (the “alpha”)
    :return:
        parameters – the updated values of the parameters object provided as input

    """
    num_layers = len(parameters) // 2

    for layer in range(1, num_layers):
        parameters["W" + str(layer)] -= learning_rate * grads["dW" + str(layer)]
        parameters["B" + str(layer)] -= learning_rate * grads["db" + str(layer)]

    return parameters


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit

    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return:
        dZ – gradient of the cost with respect to Z
    """
    x = activation_cache
    dA[x > 0] = 1
    dA[x <= 0] = 0
    return dA


def sigmoid_backward(dA, activation_cache):
    """
    Implements backward propagation for a sigmoid unit

    :param dA: The post-activation gradient
    :param activation_cache: Contains Z (stored during the forward propagation)
    :return:
        dZ – gradient of the cost with respect to Z
    """
    # x = activation_cache
    return dA * (1 - dA)
