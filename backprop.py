import numpy as np
from feedforward import ActivationFunctionNotFound


def batchnorm_backward(dZ, bn_cache):
    Z, Znorm, mu, var = bn_cache

    N, D = dZ.shape

    Z_mu = Z - mu
    std_inv = 1. / np.sqrt(var + 1e-8)

    dvar = np.sum(dZ * Z_mu, axis=0) * -.5 * std_inv ** 3
    dmu = np.sum(dZ * -std_inv, axis=0) + dvar * np.mean(-2. * Z_mu, axis=0)

    dZ_BN = (dZ * std_inv) + (dvar * 2 * Z_mu / N) + (dmu / N)

    return dZ_BN


def dropout_backward(dA, A_mask):
    return dA * A_mask


def linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer

    :param dZ: The gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: Tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    dA_prev = np.dot(W.T, dZ)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation, use_batchnorm, dropout_rate):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function
    first computes dZ and then applies the linear_backward function.

    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: which activation function was used in feedforward
    :param use_batchnorm: A boolean flag used to determine whether to apply batchnorm after the activation
    :param dropout_rate: The random percentage of neurons to dropout at each layer. 0 means no dropout.
    :return:
        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache, bn_cache, dropout_cache = cache
    use_dropout = dropout_rate > 0
    if use_dropout:
        dA = dropout_backward(dA, dropout_cache)

    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
    else:
        raise ActivationFunctionNotFound

    if use_batchnorm:
        dZ = batchnorm_backward(dZ, bn_cache)

    return linear_backward(dZ, linear_cache)


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit

    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return:
        dZ – gradient of the cost with respect to Z
    """
    relu_grad = activation_cache > 0
    dZ = dA * relu_grad

    return dZ


def L_model_backward(AL, Y, caches, use_batchnorm=0, dropout_rate=0):
    """
    Implement the backward propagation process for the entire network.

    :param AL: The probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: The true labels vector (the "ground truth" - true classifications)
    :param caches: List of caches for each layer
    :param use_batchnorm: A boolean flag used to determine whether to apply batchnorm after the activation
    :param dropout_rate: The random percentage of neurons to dropout at each layer. 0 means no dropout.
    :return:
        Grads - a dictionary with the gradients
    """

    grads = {}
    dAL = (AL - Y)
    num_of_layers = len(caches)

    grads["dA" + str(num_of_layers)], \
    grads["dW" + str(num_of_layers)], \
    grads["db" + str(num_of_layers)] = linear_backward(dAL, caches[num_of_layers - 1][0])

    for l in reversed(range(num_of_layers - 1)):
        A_prev, dW, db = linear_activation_backward(grads["dA" + str(l + 2)], caches[l], "relu",
                                                    use_batchnorm, dropout_rate)
        grads["dA" + str(l + 1)] = A_prev
        grads["dW" + str(l + 1)] = dW
        grads["db" + str(l + 1)] = db

    return grads


def update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent

    :param parameters: A python dictionary containing the DNN architecture’s parameters
    :param grads: A python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: The learning rate used to update the parameters (the “alpha”)
    :return:
        parameters – the updated values of the parameters object provided as input

    """
    num_layers = len(parameters) // 2

    for layer in range(1, num_layers + 1):
        parameters["W" + str(layer)] -= learning_rate * grads["dW" + str(layer)]
        parameters["B" + str(layer)] -= learning_rate * grads["db" + str(layer)]

    return parameters
